{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Model VGG - Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import params.consts as consts\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando para serem exibidas apenas mensagens de erro no Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Configurando para serem exibidas apenas mensagens de erro no Tensor Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = keras.datasets.cifar10 # Armazenando o dataset em uma variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = df.load_data() # Armazenando os dados do df já divididos em X e Y de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    "] # Definindo uma legenda para as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo o modelo Keras a partir de um pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(consts.VGG)) # Exibindo a referência VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa inicial\n",
    "model = keras.models.Sequential() # Instanciando o modelo\n",
    "\n",
    "# Etapas de pré processamento\n",
    "model.add(keras.layers.InputLayer(input_shape=(32, 32, 3))) # Passando o shape dos dados para o modelo\n",
    "model.add(keras.layers.Rescaling(scale=1./255)) # Fazendo o rescaling dos dados entre 0 e 255 para 0 e 1\n",
    "\n",
    "# Etapas de data augmentation\n",
    "model.add(keras.layers.RandomRotation(0.1)) # Aplicando o Data Augmentation para aumento dos dados, nesse caso rotação da imagem\n",
    "model.add(keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1)) # Aplicando o Data Augmentation para aumento dos dados, nesse caso translação da imagem\n",
    "model.add(keras.layers.RandomZoom(0.1)) # Aplicando o Data Augmentation para aumento dos dados, nesse caso zoom na imagem\n",
    "model.add(keras.layers.RandomFlip('horizontal')) # Aplicando o Data Augmentation para aumento dos dados, nesse caso flip na imagem somente na horizontal, pois o modelo não está sendo treinando com imagens de cabeça para baixo\n",
    "\n",
    "# Etapas de CNN (Usando como base conceitos de Image....., adaptado para um modelo menos complexo)\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.MaxPooling2D()) # Adicionando o Max Pooling visando desconsiderar o fundo da imagem\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.MaxPooling2D()) # Adicionando o Max Pooling visando desconsiderar o fundo da imagem\n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(128, (1, 1), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.MaxPooling2D()) # Adicionando o Max Pooling visando desconsiderar o fundo da imagem\n",
    "model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(256, (1, 1), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.MaxPooling2D()) # Adicionando o Max Pooling visando desconsiderar o fundo da imagem\n",
    "model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.Conv2D(256, (1, 1), activation='relu', padding='same')) # Adicionando uma camada convolucional\n",
    "model.add(keras.layers.MaxPooling2D()) # Adicionando o Max Pooling visando desconsiderar o fundo da imagem\n",
    "\n",
    "# Etapas de camadas ocultas\n",
    "model.add(keras.layers.Flatten()) # Realizando a redução de dimensionalidade/achatamento\n",
    "model.add(keras.layers.Dense(512, activation='relu')) # Passando a camada de sáida\n",
    "model.add(keras.layers.Dropout(0.5)) # Desligando alguns neurônios aleatóriamente para tentar reduzir o overfitting\n",
    "model.add(keras.layers.Dense(512, activation='relu')) # Passando a camada de sáida\n",
    "model.add(keras.layers.Dropout(0.5)) # Desligando alguns neurônios aleatóriamente para tentar reduzir o overfitting\n",
    "model.add(keras.layers.Dense(10, activation='softmax')) # Passando a camada de sáida (10 = valores de resultados possíveis (0 a 9))\n",
    "\n",
    "model.summary() # Exibindo o resumo do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=1E-4) # Definindo o otimizador\n",
    "loss = keras.losses.SparseCategoricalCrossentropy() # Definindo a função de busca\n",
    "metric = keras.metrics.SparseCategoricalAccuracy() # Definindo a métrica a ser considerada durante o treinamento\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric]) # Compilando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping( # Configurando um callback que interrompe o treinamento do modelo caso o desempenho pare de melhorar, com base em métricas monitoradas\n",
    "    patience = 5, # Definindo o patience, que é o número de epochs consecutivas em que a métrica monitorada pode não melhorar antes de interromper o treinamento\n",
    "    verbose = 1, # Definindo o verbose para exibir a informação de quando o callback for acionado caso aconteça\n",
    "    min_delta = 1E-4, # Definindo a menor diferença aceitável para considerar que houve melhora em uma métrica monitorada\n",
    "    start_from_epoch = 150 # Definindo a epoch mínimo em que o early stop será executado\n",
    ")\n",
    "\n",
    "history = model.fit( # Treinando o modelo e armazenando o seu resultado em uma variável\n",
    "    x_train, # Passando os valores de x de treino\n",
    "    y_train, # Passando os valores de y de treino\n",
    "    epochs = 300, # Definindo o número máximo de epochs, ou seja, quantas vezes o modelo passará por todo o conjunto de treino durante o ajuste\n",
    "    batch_size = 256, # Definindo o batch size, que é o número de amostras processadas de uma só vez antes de atualizar os pesos do modelo\n",
    "    validation_split = 0.2, # Definindo a proporção dos dados de treino que será separada para validação\n",
    "    callbacks = [early_stop] # Definindo o callback com a condição de parada definida anteriormente para interromper o treinamento do modelo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo o evaluate do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test) # Fazendo o evaluate do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando possíveis condições de overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss') # Plotando os valores de treino\n",
    "plt.plot(history.history['val_loss'], label='val_loss') # Plotando os valores de validação\n",
    "plt.legend() # Exibindo as legendas\n",
    "plt.show() # Exibindo o gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy') # Plotando os valores de treino\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label='val_accuracy') # Plotando os valores de validação\n",
    "plt.legend() # Exibindo as legendas\n",
    "plt.show() # Exibindo o gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse cenário, após a aplicação do pré processamento de Data Augmentation, os índicios de overfitting não aparecem mais, deixando assim o modelo pronto para ser utilizado com os dados reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo o predict do modelo com uma amostra dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12,6), nrows=4, ncols=5) # Definindo o tamanho da figura para exibir os gráficos\n",
    "\n",
    "for i, ax in enumerate(axs.flat): # Criando uma estrutura de repetição para percorrer cada valor de x e plotar em um gráfico\n",
    "    ax.imshow(x_test[i]) # Plotando a imagem em um gráfico em escala de cinza\n",
    "    ax.set_title(f'Class: {class_names[y_test[i][0]]}', size=10, pad=15) # Definindo o título do gráfico\n",
    "    ax.axis('off') # Desativando os títulos dos eixos\n",
    "    pred = model.predict(np.expand_dims(x_test[i], axis=0), verbose=0)[0] # Armazenando a previsão do modelo em uma variável\n",
    "    ax.text( # Adicionando um texto com a previsão do modelo\n",
    "        16, # Definindo a posição horizontal\n",
    "        -4, # Definindo a posição vertical\n",
    "        f'Predict: {class_names[pred.argmax()]} - {pred.max():.1%}', # Definindo o conteúdo do texto\n",
    "        color='green' if pred.argmax() == y_test[i] else 'red', # Definindo a cor do texto, variando de acordo com a previsão certa ou errada\n",
    "        verticalalignment = 'center', # Definindo o alinhamento vertical\n",
    "        horizontalalignment = 'center', # Definindo o alinhamento horizontal\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6) # Ajustando o espaço entre cada subfigura\n",
    "\n",
    "plt.show() # Exibindo os gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o modelo em imagens criadas manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_dir = Path('../images/real_images') # Armazenando em uma variável o caminho da pasta das imagens criadas manualmente\n",
    "\n",
    "all_pred = {} # Criando um dicionário vazio para armazenar as previsões do modelo e poder verificar as probabilidades de cada resultado posteriormente\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12,6), nrows=4, ncols=5) # Definindo o tamanho da figura para exibir os gráficos\n",
    "\n",
    "for i, (img, ax) in enumerate(zip(sorted(real_images_dir.glob('*.jpg')), axs.flat)): # Criando uma estrutura de repetição para percorrer as imagens criadas e plotar em um gráfico\n",
    "    \n",
    "    name = img.name.split('.')[0] # Armazenando em uma variável o nome da imagem com o número correto a partir do . como separador\n",
    "\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        img, target_size=(32, 32), color_mode='rgb', keep_aspect_ratio=False\n",
    "    ) # Ajustando a escala da imagem, pois o modelo foi treinado com imagens com 28 x 28 pixels, e as que fizemos possuem 128 x 128\n",
    "    img_array = keras.preprocessing.image.img_to_array(img, dtype=np.uint8) # Convertendo a imagem para um array NumPy\n",
    "    img_array = tf.expand_dims(img_array, 0) # Adicionando uma dimensão extra\n",
    "\n",
    "    pred = model.predict(img_array, verbose=0) # Armazenando o valor previsto em uma variável\n",
    "    all_pred[name] = pred # Adicionando a previsão no dicionário criado anteriormente para isso\n",
    "    \n",
    "    ax.imshow(img_array[0]) # Plotando a imagem em um gráfico em escala de cinza\n",
    "    ax.set_title(f'Class: {name}', size=10, pad=15) # Definindo o título do gráfico\n",
    "    ax.axis('off') # Desativando os títulos dos eixos\n",
    "    pred = model.predict(np.expand_dims(x_test[i], axis=0), verbose=0)[0] # Armazenando a previsão do modelo em uma variável\n",
    "    ax.text( # Adicionando um texto com a previsão do modelo\n",
    "        16, # Definindo a posição horizontal\n",
    "        -4, # Definindo a posição vertical\n",
    "        f'Predict: {class_names[pred.argmax()]} - {pred.max():.1%}', # Definindo o conteúdo do texto\n",
    "        color='green' if pred.argmax() == class_names.index(name.split('_')[0]) else 'red', # Definindo a cor do texto, variando de acordo com a previsão certa ou errada\n",
    "        verticalalignment = 'center', # Definindo o alinhamento vertical\n",
    "        horizontalalignment = 'center', # Definindo o alinhamento horizontal\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6) # Ajustando o espaço entre cada subfigura\n",
    "\n",
    "plt.show() # Exibindo os gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando as probabilidades de resultados para cada imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_pred = { # Criando um dicionário para formatar os valores \n",
    "    key: np.array([['{:.1f}%'.format(x * 100) for x in row] for row in value]) \n",
    "    for key, value in all_pred.items()\n",
    "}\n",
    "\n",
    "for key, value in formatted_pred.items(): # Criando uma estrutura de repetição para printar os resultados de chave e valor das previsões\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(consts.MODEL_DEEP_LEARNING) # Salvando o modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
