{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Model VGG - Transfer Learning e Fine_Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import params.consts as consts\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurando para serem exibidas apenas mensagens de erro no Tensor Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Configurando para serem exibidas apenas mensagens de erro no Tensor Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = keras.datasets.cifar10 # Armazenando o dataset em uma variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = df.load_data() # Armazenando os dados do df já divididos em X e Y de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    "] # Definindo uma legenda para as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo o modelo Keras a partir de um pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(consts.VGG)) # Exibindo a referência VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Configurações utilizadas ###############\n",
    "\n",
    "PASTA_IMAGENS = '../images/real_images'\n",
    "\n",
    "# Primeiro treinamento\n",
    "LEARNING_RATE = 1E-3\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Fine_tuning\n",
    "LEARNING_RATE_TUNING = 1E-3\n",
    "EPOCHS_TUNING = 15\n",
    "BATCH_SIZE_TUNING = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input # Importando o modelo treinado VGG19 e as etapas de pré processamento necessárias\n",
    "\n",
    "base_model = VGG19(weights = 'imagenet', include_top = False, input_shape = (32, 32, 3)) # Armazenando o modelo VGG19 em uma variável\n",
    "\n",
    "base_model.trainable = False # Definindo que não é para esse modelo ser treinado\n",
    "\n",
    "base_model.summary() # Exibindo o resumo do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa inicial\n",
    "model = keras.models.Sequential() # Instanciando o modelo\n",
    "\n",
    "# Etapas de pré processamento\n",
    "model.add(keras.layers.InputLayer(input_shape=(32, 32, 3))) # Passando o shape dos dados para o modelo\n",
    "\n",
    "# Etapas de transferência de aprendizado\n",
    "model.add(base_model) # Passando o modelo VGG19 já treinado\n",
    "\n",
    "# Etapas de camadas ocultas\n",
    "model.add(keras.layers.GlobalMaxPooling2D()) # Realizando a redução de dimensionalidade/achatamento\n",
    "model.add(keras.layers.Dense(4096, activation='relu')) # Passando a camada de sáida\n",
    "model.add(keras.layers.Dropout(0.5)) # Desligando alguns neurônios aleatóriamente para tentar reduzir o overfitting\n",
    "model.add(keras.layers.Dense(4096, activation='relu')) # Passando a camada de sáida\n",
    "model.add(keras.layers.Dropout(0.5)) # Desligando alguns neurônios aleatóriamente para tentar reduzir o overfitting\n",
    "model.add(keras.layers.Dense(10, activation='softmax')) # Passando a camada de sáida (10 = valores de resultados possíveis (0 a 9))\n",
    "\n",
    "model.summary(show_trainable=True) # Exibindo o resumo do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(LEARNING_RATE) # Definindo o otimizador\n",
    "loss = keras.losses.SparseCategoricalCrossentropy() # Definindo a função de busca\n",
    "metric = keras.metrics.SparseCategoricalAccuracy() # Definindo a métrica a ser considerada durante o treinamento\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric]) # Compilando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obs:** como o tempo de treinamento do modelo estava extremamente alto, tive que fazer alterações: epochs de 300 para 10 e start_from_epoch de 150 para 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocess = preprocess_input(x_train) # Fazendo o pré processamento no x de treino\n",
    "x_test_preprocess = preprocess_input(x_test) # Fazendo o pré processamento no x de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping( # Configurando um callback que interrompe o treinamento do modelo caso o desempenho pare de melhorar, com base em métricas monitoradas\n",
    "    patience = 5, # Definindo o patience, que é o número de epochs consecutivas em que a métrica monitorada pode não melhorar antes de interromper o treinamento\n",
    "    verbose = 1, # Definindo o verbose para exibir a informação de quando o callback for acionado caso aconteça\n",
    "    min_delta = 1E-4, # Definindo a menor diferença aceitável para considerar que houve melhora em uma métrica monitorada\n",
    "    start_from_epoch = 1 # Definindo a epoch mínimo em que o early stop será executado\n",
    ")\n",
    "\n",
    "history = model.fit( # Treinando o modelo e armazenando o seu resultado em uma variável\n",
    "    x_train_preprocess, # Passando os valores de x de treino\n",
    "    y_train, # Passando os valores de y de treino\n",
    "    epochs = EPOCHS, # Definindo o número máximo de epochs, ou seja, quantas vezes o modelo passará por todo o conjunto de treino durante o ajuste\n",
    "    batch_size = BATCH_SIZE, # Definindo o batch size, que é o número de amostras processadas de uma só vez antes de atualizar os pesos do modelo\n",
    "    validation_split = 0.2, # Definindo a proporção dos dados de treino que será separada para validação\n",
    "    # callbacks = [early_stop] # Definindo o callback com a condição de parada definida anteriormente para interromper o treinamento do modelo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo o evaluate do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_preprocess, y_test) # Fazendo o evaluate do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando possíveis condições de overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss') # Plotando os valores de treino\n",
    "plt.plot(history.history['val_loss'], label='val_loss') # Plotando os valores de validação\n",
    "plt.legend() # Exibindo as legendas\n",
    "plt.show() # Exibindo o gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy') # Plotando os valores de treino\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label='val_accuracy') # Plotando os valores de validação\n",
    "plt.legend() # Exibindo as legendas\n",
    "plt.show() # Exibindo o gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse cenário, após a aplicação do pré processamento de Data Augmentation, os índicios de overfitting não aparecem mais, deixando assim o modelo pronto para ser utilizado com os dados reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo o predict do modelo com uma amostra dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12,6), nrows=4, ncols=5) # Definindo o tamanho da figura para exibir os gráficos\n",
    "\n",
    "for i, ax in enumerate(axs.flat): # Criando uma estrutura de repetição para percorrer cada valor de x e plotar em um gráfico\n",
    "    ax.imshow(x_test[i]) # Plotando a imagem em um gráfico em escala de cinza\n",
    "    ax.set_title(f'Class: {class_names[y_test[i][0]]}', size=10, pad=15) # Definindo o título do gráfico\n",
    "    ax.axis('off') # Desativando os títulos dos eixos\n",
    "    pred = model.predict(np.expand_dims(x_test_preprocess[i], axis=0), verbose=0)[0] # Armazenando a previsão do modelo em uma variável\n",
    "    ax.text( # Adicionando um texto com a previsão do modelo\n",
    "        16, # Definindo a posição horizontal\n",
    "        -4, # Definindo a posição vertical\n",
    "        f'Predict: {class_names[pred.argmax()]} - {pred.max():.1%}', # Definindo o conteúdo do texto\n",
    "        color='green' if pred.argmax() == y_test[i] else 'red', # Definindo a cor do texto, variando de acordo com a previsão certa ou errada\n",
    "        verticalalignment = 'center', # Definindo o alinhamento vertical\n",
    "        horizontalalignment = 'center', # Definindo o alinhamento horizontal\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6) # Ajustando o espaço entre cada subfigura\n",
    "\n",
    "plt.show() # Exibindo os gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o modelo em imagens criadas manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_dir = Path(PASTA_IMAGENS) # Armazenando em uma variável o caminho da pasta das imagens criadas manualmente\n",
    "\n",
    "all_pred = {} # Criando um dicionário vazio para armazenar as previsões do modelo e poder verificar as probabilidades de cada resultado posteriormente\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12,6), nrows=4, ncols=5) # Definindo o tamanho da figura para exibir os gráficos\n",
    "\n",
    "for i, (img, ax) in enumerate(zip(sorted(real_images_dir.glob('*.jpg')), axs.flat)): # Criando uma estrutura de repetição para percorrer as imagens criadas e plotar em um gráfico\n",
    "    \n",
    "    name = img.name.split('.')[0] # Armazenando em uma variável o nome da imagem com o número correto a partir do . como separador\n",
    "\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        img, target_size=(32, 32), color_mode='rgb', keep_aspect_ratio=False\n",
    "    ) # Ajustando a escala da imagem, pois o modelo foi treinado com imagens com 28 x 28 pixels, e as que fizemos possuem 128 x 128\n",
    "    img_array = keras.preprocessing.image.img_to_array(img, dtype=np.uint8) # Convertendo a imagem para um array NumPy\n",
    "    img_array = tf.expand_dims(img_array, 0) # Adicionando uma dimensão extra\n",
    "\n",
    "    img_array_preprocess = preprocess_input(img_array)\n",
    "    pred = model.predict(img_array_preprocess, verbose=0) # Armazenando o valor previsto em uma variável\n",
    "    all_pred[name] = pred # Adicionando a previsão no dicionário criado anteriormente para isso\n",
    "    \n",
    "    ax.imshow(img_array[0]) # Plotando a imagem em um gráfico em escala de cinza\n",
    "    ax.set_title(f'Class: {name}', size=10, pad=15) # Definindo o título do gráfico\n",
    "    ax.axis('off') # Desativando os títulos dos eixos\n",
    "    pred = model.predict(np.expand_dims(x_test[i], axis=0), verbose=0)[0] # Armazenando a previsão do modelo em uma variável\n",
    "    ax.text( # Adicionando um texto com a previsão do modelo\n",
    "        16, # Definindo a posição horizontal\n",
    "        -4, # Definindo a posição vertical\n",
    "        f'Predict: {class_names[pred.argmax()]} - {pred.max():.1%}', # Definindo o conteúdo do texto\n",
    "        color='green' if pred.argmax() == class_names.index(name.split('_')[0]) else 'red', # Definindo a cor do texto, variando de acordo com a previsão certa ou errada\n",
    "        verticalalignment = 'center', # Definindo o alinhamento vertical\n",
    "        horizontalalignment = 'center', # Definindo o alinhamento horizontal\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6) # Ajustando o espaço entre cada subfigura\n",
    "\n",
    "plt.show() # Exibindo os gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando as probabilidades de resultados para cada imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_pred = { # Criando um dicionário para formatar os valores \n",
    "    key: np.array([['{:.1f}%'.format(x * 100) for x in row] for row in value]) \n",
    "    for key, value in all_pred.items()\n",
    "}\n",
    "\n",
    "for key, value in formatted_pred.items(): # Criando uma estrutura de repetição para printar os resultados de chave e valor das previsões\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando Fine-Tuning ao modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True # Definindo que não é para esse modelo ser treinado\n",
    "\n",
    "base_model.summary(show_trainable = True) # Exibindo o resumo do treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE_TUNING) # Definindo o otimizador\n",
    "loss = keras.losses.SparseCategoricalCrossentropy() # Definindo a função de busca\n",
    "metric = keras.metrics.SparseCategoricalAccuracy() # Definindo a métrica a ser considerada durante o treinamento\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric]) # Compilando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit( # Treinando o modelo e armazenando o seu resultado em uma variável\n",
    "    x_train_preprocess, # Passando os valores de x de treino\n",
    "    y_train, # Passando os valores de y de treino\n",
    "    epochs = EPOCHS_TUNING, # Definindo o número máximo de epochs, ou seja, quantas vezes o modelo passará por todo o conjunto de treino durante o ajuste\n",
    "    batch_size = BATCH_SIZE_TUNING, # Definindo o batch size, que é o número de amostras processadas de uma só vez antes de atualizar os pesos do modelo\n",
    "    validation_split = 0.2, # Definindo a proporção dos dados de treino que será separada para validação\n",
    "    # callbacks = [early_stop] # Definindo o callback com a condição de parada definida anteriormente para interromper o treinamento do modelo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_preprocess, y_test) # Fazendo o evaluate do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12,6), nrows=4, ncols=5) # Definindo o tamanho da figura para exibir os gráficos\n",
    "\n",
    "for i, ax in enumerate(axs.flat): # Criando uma estrutura de repetição para percorrer cada valor de x e plotar em um gráfico\n",
    "    ax.imshow(x_test[i]) # Plotando a imagem em um gráfico em escala de cinza\n",
    "    ax.set_title(f'Class: {class_names[y_test[i][0]]}', size=10, pad=15) # Definindo o título do gráfico\n",
    "    ax.axis('off') # Desativando os títulos dos eixos\n",
    "    pred = model.predict(np.expand_dims(x_test_preprocess[i], axis=0), verbose=0)[0] # Armazenando a previsão do modelo em uma variável\n",
    "    ax.text( # Adicionando um texto com a previsão do modelo\n",
    "        16, # Definindo a posição horizontal\n",
    "        -4, # Definindo a posição vertical\n",
    "        f'Predict: {class_names[pred.argmax()]} - {pred.max():.1%}', # Definindo o conteúdo do texto\n",
    "        color='green' if pred.argmax() == y_test[i] else 'red', # Definindo a cor do texto, variando de acordo com a previsão certa ou errada\n",
    "        verticalalignment = 'center', # Definindo o alinhamento vertical\n",
    "        horizontalalignment = 'center', # Definindo o alinhamento horizontal\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6) # Ajustando o espaço entre cada subfigura\n",
    "\n",
    "plt.show() # Exibindo os gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_dir = Path(PASTA_IMAGENS) # Armazenando em uma variável o caminho da pasta das imagens criadas manualmente\n",
    "\n",
    "all_pred = {} # Criando um dicionário vazio para armazenar as previsões do modelo e poder verificar as probabilidades de cada resultado posteriormente\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12,6), nrows=4, ncols=5) # Definindo o tamanho da figura para exibir os gráficos\n",
    "\n",
    "for i, (img, ax) in enumerate(zip(sorted(real_images_dir.glob('*.jpg')), axs.flat)): # Criando uma estrutura de repetição para percorrer as imagens criadas e plotar em um gráfico\n",
    "    \n",
    "    name = img.name.split('.')[0] # Armazenando em uma variável o nome da imagem com o número correto a partir do . como separador\n",
    "\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        img, target_size=(32, 32), color_mode='rgb', keep_aspect_ratio=False\n",
    "    ) # Ajustando a escala da imagem, pois o modelo foi treinado com imagens com 28 x 28 pixels, e as que fizemos possuem 128 x 128\n",
    "    img_array = keras.preprocessing.image.img_to_array(img, dtype=np.uint8) # Convertendo a imagem para um array NumPy\n",
    "    img_array = tf.expand_dims(img_array, 0) # Adicionando uma dimensão extra\n",
    "\n",
    "    img_array_preprocess = preprocess_input(img_array)\n",
    "    pred = model.predict(img_array_preprocess, verbose=0) # Armazenando o valor previsto em uma variável\n",
    "    all_pred[name] = pred # Adicionando a previsão no dicionário criado anteriormente para isso\n",
    "    \n",
    "    ax.imshow(img_array[0]) # Plotando a imagem em um gráfico em escala de cinza\n",
    "    ax.set_title(f'Class: {name}', size=10, pad=15) # Definindo o título do gráfico\n",
    "    ax.axis('off') # Desativando os títulos dos eixos\n",
    "    pred = model.predict(np.expand_dims(x_test[i], axis=0), verbose=0)[0] # Armazenando a previsão do modelo em uma variável\n",
    "    ax.text( # Adicionando um texto com a previsão do modelo\n",
    "        16, # Definindo a posição horizontal\n",
    "        -4, # Definindo a posição vertical\n",
    "        f'Predict: {class_names[pred.argmax()]} - {pred.max():.1%}', # Definindo o conteúdo do texto\n",
    "        color='green' if pred.argmax() == class_names.index(name.split('_')[0]) else 'red', # Definindo a cor do texto, variando de acordo com a previsão certa ou errada\n",
    "        verticalalignment = 'center', # Definindo o alinhamento vertical\n",
    "        horizontalalignment = 'center', # Definindo o alinhamento horizontal\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6) # Ajustando o espaço entre cada subfigura\n",
    "\n",
    "plt.show() # Exibindo os gráficos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
